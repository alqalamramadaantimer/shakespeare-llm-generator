{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alqalamramadaantimer/shakespeare-llm-generator/blob/main/GenaShakespeare-llm-generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Install PyTorch and NumPy\n",
        "!pip install -q torch numpy\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "# Set seed for reproducibility\n",
        "torch.manual_seed(42)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQjfn4laeY78",
        "outputId": "838ca2e5-4b32-47b1-cb09-e3e1f19aa32c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m84.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7be64432b090>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Load the custom Shakespeare poetry file\n",
        "with open(\"/content/shakespeare_poetry.txt\", 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(\"ğŸ“„ Dataset sample:\")\n",
        "print(text[:500])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_oCf8xFfoLV",
        "outputId": "cbaddf9b-0b2b-4df5-c3a0-2d82a8fbda9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“„ Dataset sample:\n",
            "SONNET 18  \n",
            "Shall I compare thee to a summerâ€™s day?  \n",
            "Thou art more lovely and more temperate:  \n",
            "Rough winds do shake the darling buds of May,  \n",
            "And summerâ€™s lease hath all too short a date:  \n",
            "Sometime too hot the eye of heaven shines,  \n",
            "And often is his gold complexion dimmed;  \n",
            "And every fair from fair sometime declines,  \n",
            "By chance or natureâ€™s changing course untrimmed;  \n",
            "But thy eternal summer shall not fade,  \n",
            "Nor lose possession of that fair thou owest;  \n",
            "Nor shall Death brag thou wanderes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Tokenization and vocabulary mapping\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "stoi = {ch: i for i, ch in enumerate(chars)}\n",
        "itos = {i: ch for ch, i in stoi.items()}\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "block_size = 64  # More context for better generation\n"
      ],
      "metadata": {
        "id": "y0zvfR7LoMFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Generate training batches\n",
        "def get_batch(batch_size=16):\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n"
      ],
      "metadata": {
        "id": "jGTkb845oOTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: Define the model\n",
        "class MiniGPT(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.embed = nn.Embedding(vocab_size, 32)\n",
        "        self.lstm = nn.LSTM(32, 64, batch_first=True)\n",
        "        self.fc = nn.Linear(64, vocab_size)\n",
        "\n",
        "    def forward(self, x, targets=None):\n",
        "        x = self.embed(x)\n",
        "        out, _ = self.lstm(x)\n",
        "        logits = self.fc(out)\n",
        "\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n"
      ],
      "metadata": {
        "id": "Ysw9CWILoSSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 6: Train the model\n",
        "model = MiniGPT()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "for step in range(1000):\n",
        "    xb, yb = get_batch()\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if step % 100 == 0:\n",
        "        print(f\"Step {step} | Loss: {loss.item():.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_QFgZsEeoanQ",
        "outputId": "f0db9b85-be5d-4c7b-cfca-99639c0bea08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 0 | Loss: 4.1023\n",
            "Step 100 | Loss: 1.8050\n",
            "Step 200 | Loss: 1.1472\n",
            "Step 300 | Loss: 0.8364\n",
            "Step 400 | Loss: 0.6430\n",
            "Step 500 | Loss: 0.4650\n",
            "Step 600 | Loss: 0.3678\n",
            "Step 700 | Loss: 0.3697\n",
            "Step 800 | Loss: 0.2942\n",
            "Step 900 | Loss: 0.2279\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 7: Text generator function\n",
        "def generate(model, start_text=\"Shall I compare thee\", max_new_tokens=200):\n",
        "    model.eval()\n",
        "    idx = torch.tensor(encode(start_text), dtype=torch.long)[None, :]\n",
        "    for _ in range(max_new_tokens):\n",
        "        logits, _ = model(idx[:, -block_size:])\n",
        "        next_logits = logits[:, -1, :]\n",
        "        probs = F.softmax(next_logits, dim=-1)\n",
        "        next_id = torch.multinomial(probs, num_samples=1)\n",
        "        idx = torch.cat([idx, next_id], dim=1)\n",
        "    return decode(idx[0].tolist())\n"
      ],
      "metadata": {
        "id": "59CZ4u7_olQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 8: Generate text and check for understandable English\n",
        "def generate_valid_poetry(model, start_text=\"Shall I compare thee\", max_new_tokens=200, threshold=60):\n",
        "    print(\"ğŸ”® Generating Shakespeare-style poetry...\")\n",
        "    poem = generate(model, start_text, max_new_tokens)\n",
        "\n",
        "    readability = textstat.flesch_reading_ease(poem)\n",
        "    print(\"\\nğŸ“Š Readability Score:\", round(readability, 2), \"/ 100\")\n",
        "\n",
        "    if readability >= threshold:\n",
        "        print(\"âœ… Output is clear and understandable English.\\n\")\n",
        "        print(\"ğŸ“ Final Generated Poetry:\\n\")\n",
        "        print(poem)\n",
        "        return poem\n",
        "    else:\n",
        "        print(\"âš ï¸ Warning: The generated text may not be very readable.\\n\")\n",
        "        print(\"ğŸ“„ Raw Output:\\n\")\n",
        "        print(poem)\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "DQptvb8SomfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Text generator function\n",
        "def generate(model, start_text, max_new_tokens=200):\n",
        "    model.eval()\n",
        "    idx = torch.tensor(encode(start_text), dtype=torch.long)[None, :]\n",
        "    for _ in range(max_new_tokens):\n",
        "        logits, _ = model(idx[:, -block_size:])\n",
        "        next_logits = logits[:, -1, :]\n",
        "        probs = F.softmax(next_logits, dim=-1)\n",
        "        next_id = torch.multinomial(probs, num_samples=1)\n",
        "        idx = torch.cat([idx, next_id], dim=1)\n",
        "    return decode(idx[0].tolist())\n",
        "\n",
        "# Try generating\n",
        "print(\"ğŸ“ Generated Text:\\n\")\n",
        "print(generate(model, \"Shall I compare thee\", max_new_tokens=200))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfHP29bWpEMe",
        "outputId": "31cb9e78-fe4b-4009-d2cb-713325660e65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ“ Generated Text:\n",
            "\n",
            "Shall I compare thee.  \n",
            "\n",
            "SONNET 130  \n",
            "My mistress when she walks treads on that fair thou owef sheet abort etime to every heaven shines to time the Coldes,  \n",
            "But bears like and more temperate:  \n",
            "Rough winds do shath a da\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# STEP 9: Generate and verify output\n",
        "# Reinstall and import textstat\n",
        "!pip install -q textstat\n",
        "import textstat\n",
        "\n",
        "generate_valid_poetry(model, start_text=\"Love is\", max_new_tokens=200)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 278
        },
        "id": "8qCCO0WPHa2y",
        "outputId": "52ebe7cb-8667-41b4-ac28-853f489ed917"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”® Generating Shakespeare-style poetry...\n",
            "\n",
            "ğŸ“Š Readability Score: 70.31 / 100\n",
            "âœ… Output is clear and understandable English.\n",
            "\n",
            "ğŸ“ Final Generated Poetry:\n",
            "\n",
            "Love is fal some:  \n",
            "And yet, by heaven, I this gives ldines to time thou growest:  \n",
            "So long and wighteâ€™s not  \n",
            "The fieldâ€™s chief his gold changing sickleâ€™s no hing to sich his shade the chaste.  \n",
            "\n",
            "SONNET 130\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Love is fal some:  \\nAnd yet, by heaven, I this gives ldines to time thou growest:  \\nSo long and wighteâ€™s not  \\nThe fieldâ€™s chief his gold changing sickleâ€™s no hing to sich his shade the chaste.  \\n\\nSONNET 130'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    }
  ]
}